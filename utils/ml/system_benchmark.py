"""
ML System Benchmark - Performans Analizi ve Test
TÃ¼m ML Ã¶zelliklerinin performansÄ±nÄ± Ã¶lÃ§er
"""

import time
import psutil
import numpy as np
import pandas as pd
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class MLSystemBenchmark:
    """ML sistem performans testi"""
    
    def __init__(self, db):
        self.db = db
        self.results = {}
    
    def measure_time_memory(self, func, *args, **kwargs):
        """Fonksiyon Ã§alÄ±ÅŸma sÃ¼resi ve bellek kullanÄ±mÄ±nÄ± Ã¶lÃ§"""
        import tracemalloc
        
        # BaÅŸlangÄ±Ã§
        tracemalloc.start()
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # Fonksiyonu Ã§alÄ±ÅŸtÄ±r
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        # BitiÅŸ
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        metrics = {
            'duration_seconds': end_time - start_time,
            'memory_used_mb': end_memory - start_memory,
            'peak_memory_mb': peak / 1024 / 1024,
            'success': success,
            'error': error
        }
        
        return result, metrics
    
    def benchmark_data_collection(self):
        """Veri toplama performansÄ±"""
        logger.info("ðŸ“Š Veri toplama benchmark baÅŸladÄ±...")
        
        # Eski sistem
        from utils.ml.data_collector import DataCollector
        old_collector = DataCollector(self.db)
        
        result, metrics = self.measure_time_memory(
            old_collector.collect_stok_metrics
        )
        
        self.results['data_collection_old'] = {
            'name': 'Veri Toplama (Eski)',
            'collected_count': result if result else 0,
            **metrics
        }
        
        # Yeni sistem
        from utils.ml.data_collector_v2 import DataCollectorV2
        new_collector = DataCollectorV2(self.db)
        
        result, metrics = self.measure_time_memory(
            new_collector.collect_stok_metrics_incremental
        )
        
        self.results['data_collection_new'] = {
            'name': 'Veri Toplama (Yeni - Incremental)',
            'collected_count': result if result else 0,
            **metrics
        }
        
        logger.info("âœ… Veri toplama benchmark tamamlandÄ±")
    
    def benchmark_feature_engineering(self):
        """Feature engineering performansÄ±"""
        logger.info("ðŸ“Š Feature engineering benchmark baÅŸladÄ±...")
        
        from utils.ml.feature_engineer import FeatureEngineer
        
        engineer = FeatureEngineer(self.db)
        
        # Tek Ã¼rÃ¼n iÃ§in
        result, metrics = self.measure_time_memory(
            engineer.extract_stok_features,
            urun_id=1,
            lookback_days=30
        )
        
        self.results['feature_engineering_single'] = {
            'name': 'Feature Engineering (Tek Entity)',
            'feature_count': len(result) if result else 0,
            **metrics
        }
        
        # TÃ¼m Ã¼rÃ¼nler iÃ§in
        result, metrics = self.measure_time_memory(
            engineer.create_feature_matrix,
            'stok_seviye',
            lookback_days=30
        )
        
        self.results['feature_engineering_matrix'] = {
            'name': 'Feature Engineering (Matrix)',
            'entity_count': len(result) if result is not None else 0,
            'feature_count': len(result.columns) if result is not None else 0,
            **metrics
        }
        
        logger.info("âœ… Feature engineering benchmark tamamlandÄ±")
    
    def benchmark_feature_selection(self):
        """Feature selection performansÄ±"""
        logger.info("ðŸ“Š Feature selection benchmark baÅŸladÄ±...")
        
        from utils.ml.feature_engineer import FeatureEngineer
        from utils.ml.feature_selector import FeatureSelector
        
        # Feature matrix oluÅŸtur
        engineer = FeatureEngineer(self.db)
        df = engineer.create_feature_matrix('stok_seviye', lookback_days=30)
        
        if df is None or len(df) < 10:
            logger.warning("Yetersiz veri, feature selection atlanÄ±yor")
            return
        
        selector = FeatureSelector()
        
        # Auto selection
        result, metrics = self.measure_time_memory(
            selector.auto_select,
            df.copy(),
            method='all'
        )
        
        self.results['feature_selection'] = {
            'name': 'Feature Selection (Auto)',
            'original_features': len(df.columns),
            'selected_features': len(result) if result else 0,
            'reduction_percent': ((len(df.columns) - len(result)) / len(df.columns) * 100) if result else 0,
            **metrics
        }
        
        logger.info("âœ… Feature selection benchmark tamamlandÄ±")
    
    def benchmark_feature_interaction(self):
        """Feature interaction performansÄ±"""
        logger.info("ðŸ“Š Feature interaction benchmark baÅŸladÄ±...")
        
        from utils.ml.feature_engineer import FeatureEngineer
        from utils.ml.feature_interaction import enhance_features_with_interactions
        
        # Feature matrix oluÅŸtur
        engineer = FeatureEngineer(self.db)
        df = engineer.create_feature_matrix('stok_seviye', lookback_days=30)
        
        if df is None or len(df) < 10:
            logger.warning("Yetersiz veri, feature interaction atlanÄ±yor")
            return
        
        original_count = len(df.columns)
        
        # Interaction ekleme
        result, metrics = self.measure_time_memory(
            enhance_features_with_interactions,
            df.copy()
        )
        
        self.results['feature_interaction'] = {
            'name': 'Feature Interaction',
            'original_features': original_count,
            'final_features': len(result.columns) if result is not None else 0,
            'added_features': (len(result.columns) - original_count) if result is not None else 0,
            **metrics
        }
        
        logger.info("âœ… Feature interaction benchmark tamamlandÄ±")
    
    def benchmark_model_training(self):
        """Model eÄŸitimi performansÄ±"""
        logger.info("ðŸ“Š Model eÄŸitimi benchmark baÅŸladÄ±...")
        
        from utils.ml.model_trainer import ModelTrainer
        from models import MLMetric
        from datetime import timedelta, timezone
        
        trainer = ModelTrainer(self.db)
        
        # Veri hazÄ±rla
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=30)
        metrics = MLMetric.query.filter(
            MLMetric.metric_type == 'stok_seviye',
            MLMetric.timestamp >= cutoff_date
        ).all()
        
        if len(metrics) < 10:
            logger.warning("Yetersiz veri, model training atlanÄ±yor")
            return
        
        data = np.array([m.metric_value for m in metrics])
        
        # Ham veri ile eÄŸitim
        result, metrics_dict = self.measure_time_memory(
            trainer.train_isolation_forest,
            'stok_seviye',
            data,
            use_feature_engineering=False
        )
        
        self.results['model_training_basic'] = {
            'name': 'Model EÄŸitimi (Ham Veri)',
            'data_points': len(data),
            'accuracy': result[3] if result and result[0] else 0,
            **metrics_dict
        }
        
        # Feature engineering ile eÄŸitim
        result, metrics_dict = self.measure_time_memory(
            trainer.train_isolation_forest,
            'stok_seviye',
            data,
            use_feature_engineering=True
        )
        
        self.results['model_training_advanced'] = {
            'name': 'Model EÄŸitimi (Feature Engineering)',
            'data_points': len(data),
            'feature_count': len(result[2]) if result and result[2] else 0,
            'accuracy': result[3] if result and result[0] else 0,
            **metrics_dict
        }
        
        logger.info("âœ… Model eÄŸitimi benchmark tamamlandÄ±")
    
    def run_full_benchmark(self):
        """TÃ¼m benchmark'larÄ± Ã§alÄ±ÅŸtÄ±r"""
        logger.info("ðŸš€ Tam sistem benchmark baÅŸladÄ±...")
        
        start_time = time.time()
        
        try:
            self.benchmark_data_collection()
        except Exception as e:
            logger.error(f"Data collection benchmark hatasÄ±: {str(e)}")
        
        try:
            self.benchmark_feature_engineering()
        except Exception as e:
            logger.error(f"Feature engineering benchmark hatasÄ±: {str(e)}")
        
        try:
            self.benchmark_feature_selection()
        except Exception as e:
            logger.error(f"Feature selection benchmark hatasÄ±: {str(e)}")
        
        try:
            self.benchmark_feature_interaction()
        except Exception as e:
            logger.error(f"Feature interaction benchmark hatasÄ±: {str(e)}")
        
        try:
            self.benchmark_model_training()
        except Exception as e:
            logger.error(f"Model training benchmark hatasÄ±: {str(e)}")
        
        total_time = time.time() - start_time
        
        logger.info(f"âœ… Tam benchmark tamamlandÄ± ({total_time:.2f} saniye)")
        
        return self.results
    
    def generate_report(self):
        """Benchmark raporu oluÅŸtur"""
        if not self.results:
            return "Benchmark henÃ¼z Ã§alÄ±ÅŸtÄ±rÄ±lmadÄ±!"
        
        report = []
        report.append("="*80)
        report.append("ML SÄ°STEM PERFORMANS RAPORU")
        report.append("="*80)
        report.append(f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        for key, result in self.results.items():
            report.append(f"\n{result['name']}")
            report.append("-"*60)
            report.append(f"  SÃ¼re: {result['duration_seconds']:.3f} saniye")
            report.append(f"  Bellek: {result['memory_used_mb']:.2f} MB")
            report.append(f"  Peak Bellek: {result['peak_memory_mb']:.2f} MB")
            report.append(f"  BaÅŸarÄ±lÄ±: {'âœ…' if result['success'] else 'âŒ'}")
            
            if not result['success']:
                report.append(f"  Hata: {result['error']}")
            
            # Ã–zel metrikler
            for k, v in result.items():
                if k not in ['name', 'duration_seconds', 'memory_used_mb', 'peak_memory_mb', 'success', 'error']:
                    report.append(f"  {k}: {v}")
        
        report.append("\n" + "="*80)
        
        return "\n".join(report)
    
    def save_report(self, filepath='benchmark_report.txt'):
        """Raporu dosyaya kaydet"""
        report = self.generate_report()
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"âœ… Rapor kaydedildi: {filepath}")
        
        return filepath


# KullanÄ±m
def run_system_benchmark():
    """Sistem benchmark'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r"""
    try:
        from models import db
        from app import app
        
        with app.app_context():
            benchmark = MLSystemBenchmark(db)
            results = benchmark.run_full_benchmark()
            
            # Rapor oluÅŸtur
            report = benchmark.generate_report()
            print(report)
            
            # Dosyaya kaydet
            benchmark.save_report('docs/benchmark_report.txt')
            
            return results
            
    except Exception as e:
        logger.error(f"Benchmark hatasÄ±: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    run_system_benchmark()
